{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488aa9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d280bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class to contain the testbed: states, actions, rewards etc\n",
    "#to incorporate question 2f too we will include an indicator that tells us if we want to use a random walk or not\n",
    "class ArmedBanditTB:\n",
    "    \n",
    "    def __init__(self, nRuns, eps, q_init = 5, ucb = False, c=2, rw = False, alpha_const = False, alpha = 0.1, decay_eps = False):\n",
    "        \n",
    "        #number of times we will interact with environment\n",
    "        self.nRuns= nRuns\n",
    "        \n",
    "        #eps for eps greedy algorithm (0 eps just means standard greedy)\n",
    "        self.epsilon = eps\n",
    "        \n",
    "        #initial values for all Q_0(a_t)\n",
    "        self.initial_q = q_init\n",
    "        \n",
    "        #if we want to use UCB or not\n",
    "        self.ucb = ucb\n",
    "        if self.ucb:\n",
    "            self.c = c\n",
    "        \n",
    "        #constant alphas for updates or not\n",
    "        self.alpha_const = alpha_const\n",
    "        if self.alpha_const:\n",
    "            self.alpha = alpha\n",
    "        \n",
    "        #implemented for part f when we want to use random walks to update our Q*(At)s\n",
    "        self.rw = rw\n",
    "        \n",
    "        self.decay = decay_eps\n",
    "        \n",
    "        #calling function below to initialise a few more variables- case specific generalisations\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        self.Q = np.ones(10)*self.initial_q\n",
    "        \n",
    "        #for the ucb calculation if needed\n",
    "        if self.ucb:\n",
    "            self.U = np.zeros(10)\n",
    "        \n",
    "        #store rewards, count of how many times each action has been taken\n",
    "        self.rewards = []\n",
    "        #conventionally initialise all counts at 1 (no division by 0 errors)\n",
    "        self.count = np.ones(10)\n",
    "        \n",
    "        #indicator of optimal move chosen or not at each step (for optimal choice ratio)\n",
    "        self.count_correct = []\n",
    "        \n",
    "        #for part 2f we know we want to update Q* at each timestep as a random walk, start at 0\n",
    "        if self.rw:\n",
    "            self.q_actuals = np.zeros(10)\n",
    "        #part 2e we will just initialise with some random normal(0,1) values\n",
    "        else:\n",
    "            self.q_actuals = np.random.normal(size=10)\n",
    "            \n",
    "        #will be required for part 2f when we want to also consider maximum reward attainable so far\n",
    "        #therefore we store the history of the Q* now as its non stationary (at each time step we can now ascertain maximum reward so far)\n",
    "        self.q_optimals_history = np.empty(shape=(10, self.nRuns))        \n",
    "    \n",
    "    #function to update Q recursively in alpha constant or not case\n",
    "    def updateQ(self,action_index, R):\n",
    "        if self.alpha_const:\n",
    "            self.Q[action_index] = self.Q[action_index] + (self.alpha * (R-self.Q[action_index]))\n",
    "        else:\n",
    "            N = 1/self.count[action_index]\n",
    "            #Q2b shows us this is how to update recursively in the sample average case\n",
    "            self.Q[action_index] = self.Q[action_index] + (N * (R-self.Q[action_index]))\n",
    "    \n",
    "    #random walk updates- only needed in random walk case\n",
    "    def updateTrueQ(self):\n",
    "        self.q_actuals += np.random.normal(loc=0.0,scale=0.1,size=10)\n",
    "    \n",
    "    #normal reward with mean Q*(index) and Variance = 1\n",
    "    def generateRewards(self, action_index):\n",
    "        return np.random.normal(loc=self.q_actuals[action_index], scale = 1)\n",
    "    \n",
    "    #will return all indices where we have greedy choice (could be more than one)\n",
    "    def findGreedy(self,arr):\n",
    "        greedy_indices = []\n",
    "        \n",
    "        #greedy choice\n",
    "        max_arr = np.max(arr)\n",
    "        \n",
    "        for i in range(len(arr)):\n",
    "            if max_arr == arr[i]:\n",
    "                greedy_indices.append(i)\n",
    "        return greedy_indices\n",
    "    \n",
    "    #function that will make greedy choice- random choice from greedya actions\n",
    "    def chooseGreedyAction(self, arr):\n",
    "        return np.random.choice(self.findGreedy(arr))\n",
    "    \n",
    "    #function to check if action made was optimal (in line with Q*) or not\n",
    "    def checkOptimal(self, action_index):\n",
    "        optimal_act = np.argmax(self.q_actuals)\n",
    "\n",
    "        if action_index == optimal_act:\n",
    "            self.count_correct.append(1)\n",
    "        else:\n",
    "            self.count_correct.append(0)\n",
    "    \n",
    "    #make move (random or greedy) and output reward from move + make updates to scheme\n",
    "    def makeMove(self, random=False):\n",
    "        \n",
    "        #if random i.e. eps case then just take a random index and return its associated reward\n",
    "        if random:\n",
    "            index = np.random.randint(10)\n",
    "        \n",
    "        #if not random want greedy choice- UCB case or standard case- call chooseGreedy function\n",
    "        else:\n",
    "            if self.ucb:\n",
    "                overall_t = np.sum(self.count)\n",
    "                self.U = self.c * np.sqrt(np.log(overall_t) / self.count)\n",
    "                arr = self.U + self.Q\n",
    "            else:\n",
    "                arr = self.Q\n",
    "\n",
    "            index = np.random.choice(self.findGreedy(arr))\n",
    "        \n",
    "        #generate associated reward\n",
    "        R = self.generateRewards(index)\n",
    "        \n",
    "        #non-stationary case first update q_actuals (only update if random walk)\n",
    "        if self.rw:\n",
    "            self.updateTrueQ()\n",
    "        \n",
    "        #update count correct if optimal or not\n",
    "        self.checkOptimal(index)\n",
    "        \n",
    "        #increment how many moves made with this index\n",
    "        self.count[index] += 1\n",
    "        \n",
    "        #make recursive update to our estimate Qs; depending on if its alpha constant or not\n",
    "        self.updateQ(index, R)\n",
    "        \n",
    "        return R\n",
    "    \n",
    "    #run trial, go through all time steps and pick epsilon greedy or greedy choice\n",
    "    def runTrials(self):\n",
    "        \n",
    "        for i in range(self.nRuns):\n",
    "            \n",
    "            #store optimal Q* at this timestep as we go along (rw case)\n",
    "            if self.rw:\n",
    "                self.q_optimals_history[:,i]=self.q_actuals\n",
    "            \n",
    "            #condition on random not needed as if eps = 0 the unif will always be >0 anyway\n",
    "            unif = np.random.random()\n",
    "            if self.decay:\n",
    "                eps = self.epsilon / (i+1)\n",
    "            else:\n",
    "                eps = self.epsilon\n",
    "            if unif < eps:\n",
    "                R = self.makeMove(random=True)\n",
    "            else:\n",
    "                R = self.makeMove(random=False)\n",
    "            \n",
    "            self.rewards.append(R)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716f03a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running part e- will initialise a 10 armed bandit as needed with 1000 timesteps and then run it 2000 times\n",
    "def playBanditStationary(runs=2000,steps=1000, eps_val = 0, q_init_val = 5, ucb_indicator = False, decay_eps = False, verbose = 1):\n",
    "    '''\n",
    "    verbose = 0: don't print runs progress, verbose > 0: print runs progress\n",
    "    Use 2000 runs as per the experiment carried out in the link provided\n",
    "    '''\n",
    "    bandit = ArmedBanditTB(nRuns=steps, eps = eps_val, q_init=q_init_val, ucb=ucb_indicator, decay_eps = decay_eps)\n",
    "    \n",
    "    rewards = np.zeros(shape=(steps,runs))\n",
    "    correct_ratio = np.zeros(shape=(steps,runs))\n",
    "    \n",
    "    for i in range(runs):\n",
    "        if verbose != 0:\n",
    "            if i == runs-1:\n",
    "                print(\"Run %i Completed\" %runs)\n",
    "            elif i == 0:\n",
    "                print(\"Run 001 Completed\")\n",
    "            elif i%250 == 0:\n",
    "                print(\"Run %i Completed\" %i)\n",
    "        \n",
    "        bandit.reset()\n",
    "        bandit.runTrials()\n",
    "        \n",
    "        rewards[:, i] = np.asarray(bandit.rewards)\n",
    "        correct_ratio[:, i] = np.asarray(bandit.count_correct)\n",
    "    \n",
    "    #provides optimal action ratio\n",
    "    correct_arr = correct_ratio.mean(axis=1)\n",
    "    \n",
    "    return rewards, correct_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9110ffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now obtaining results we want to test\n",
    "\n",
    "#greedy with Q_0 = 5, no UCB\n",
    "print(\"Greedy, Initial Values 5\")\n",
    "rewards_greedy, ratios_greedy = playBanditStationary()\n",
    "\n",
    "#epsilons we want to test against the greedy, initial values 0\n",
    "epsilons = [0.10, 0.01]\n",
    "reward_eps, ratios_eps = [], []\n",
    "for es in epsilons:\n",
    "    print(\"\\neps-Greedy with eps = %.2f, Initial Values 0, UCB\" %es)\n",
    "    rewards, ratios = playBanditStationary(eps_val=es, q_init_val= 0, ucb_indicator=True)\n",
    "    reward_eps.append(rewards)\n",
    "    ratios_eps.append(ratios)\n",
    "    \n",
    "#greedy with Q_0 = 0, no UCB (no verbose as know it works from above)\n",
    "rewards_greedy_1, ratios_greedy_1 = playBanditStationary(q_init_val = 0, verbose=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fff470",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rewards_eps_decay, ratios_decay_eps = playBanditStationary(eps_val=0.1, q_init_val= 0, ucb_indicator=True, decay_eps = True)\n",
    "rewards_eps_decay5, ratios_decay_eps5 = playBanditStationary(eps_val=0.1, q_init_val= 5, ucb_indicator=True, decay_eps = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbda11d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resulting plots- comparing Q_0 = Greedy to epsGreedy counterparts\n",
    "fig, (ax1,ax2) = plt.subplots(2,1,figsize=(10,8))\n",
    "#average rewards plot\n",
    "ax1.plot(rewards_greedy.mean(axis=1), label = 'Greedy, Initial Value 5')\n",
    "for es2 in range(len(epsilons)):\n",
    "    ax1.plot(reward_eps[es2].mean(axis=1),\n",
    "             label = 'epsGreedy with Epsilon = %.2f, Initial Value 0, UCB' %epsilons[es2])\n",
    "ax1.set_xlabel(\"Steps\",fontsize=12)\n",
    "ax1.set_ylabel(\"Average Reward\", fontsize=12)\n",
    "ax1.set_title(\"eps-Greedy vs Greedy\", fontsize=12)\n",
    "ax1.plot(rewards_eps_decay5.mean(axis=1), label = 'eps_t Decay Greedy, Initial Value 0')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "#optimal rewards plot\n",
    "ax2.plot(ratios_greedy*100, label = 'Greedy, Initial Value 5')\n",
    "for es3 in range(len(epsilons)):\n",
    "    ax2.plot(100*(ratios_eps[es3]),\n",
    "             label = 'epsGreedy with Epsilon = %.2f, Initial Value 0, UCB' %epsilons[es3])\n",
    "ax2.set_xlabel(\"Steps\",fontsize=12)\n",
    "ax2.set_ylabel(\"Optimal Action (%)\", fontsize=12)\n",
    "ax2.set_title(\"eps-Greedy vs Greedy\", fontsize=12)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"NOW USING Q_0 = 0 for ALL INSTANCES\")\n",
    "\n",
    "#now plots for Q_0 = 0 for all\n",
    "fig, (ax1_1,ax2_1) = plt.subplots(2,1,figsize=(10,8))\n",
    "#average rewards plot\n",
    "ax1_1.plot(rewards_greedy_1.mean(axis=1), label = 'Greedy, Initial Value 0')\n",
    "for es2 in range(len(epsilons)):\n",
    "    ax1_1.plot(reward_eps[es2].mean(axis=1),\n",
    "             label = 'epsGreedy with Epsilon = %.2f, Initial Value 0, UCB' %epsilons[es2])\n",
    "ax1_1.set_xlabel(\"Steps\",fontsize=12)\n",
    "ax1_1.set_ylabel(\"Average Reward\", fontsize=12)\n",
    "ax1_1.set_title(\"eps-Greedy vs Greedy\", fontsize=12)\n",
    "ax1_1.plot(rewards_eps_decay.mean(axis=1), label = 'eps_t Decay Greedy, Initial Value 0')\n",
    "ax1_1.plot(rewards_eps_decay5.mean(axis=1), label = 'eps_t Decay Greedy, Initial Value 5')\n",
    "ax1_1.legend()\n",
    "\n",
    "\n",
    "#optimal rewards plot\n",
    "ax2_1.plot(ratios_greedy_1*100, label = 'Greedy, Initial Value 0')\n",
    "for es in range(len(epsilons)):\n",
    "    ax2_1.plot(100*(ratios_eps[es]),\n",
    "             label = 'epsGreedy with Epsilon = %.2f, Initial Value 0, UCB' %epsilons[es])\n",
    "ax2_1.set_xlabel(\"Steps\",fontsize=12)\n",
    "ax2_1.set_ylabel(\"Optimal Action (%)\", fontsize=12)\n",
    "ax2_1.set_title(\"eps-Greedy vs Greedy\", fontsize=12)\n",
    "ax2_1.plot(ratios_decay_eps*100, label = 'Greedy, Initial Value 0')\n",
    "ax2_1.plot(ratios_decay_eps5*100, label = 'eps_t Greedy, Initial Value 5')\n",
    "ax2_1.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98c7a30",
   "metadata": {},
   "source": [
    "### Q2f\n",
    "\n",
    "Now initialising Q* optimal values as a random walk (starting at 0) instead of a stationary value. Therefore, at each timestep we will update Q* by adding on a Normal(0,0.01) sample to each instance. \n",
    "\n",
    "Moreover, to showcase the limitations of a sample-average method in updating Q estimates, we will compare it to an action-value method that updates Q estimates at every increment with a constant alpha (=0.1) parameter. \n",
    "\n",
    "Mostly, the experimentation and resulting plots will remain the same. The main change will be now we are also interested in the maximum attainable reward which we use to compare our two methods (can be used to showcase the sub-optimality of one in time better). This change comes in the form of now also updating the Q* history (now a random walk so max changing) and storing the max reward attainable at that time. Then we will plot all rewards and see which method (constant alpha OR sample-average) provides a reward closer to the maximum attainable reward as well as look at the optimality % as usual.\n",
    "\n",
    "Additionally, we will also use 10,000 steps instead of a 1000 in each run, Number of Runs remains at 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4786b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def playBanditNonStationary(steps=10000, runs=2000, verbose = 1):\n",
    "    '''\n",
    "    Function to carry out all the testing required for Q2f\n",
    "    First run the standard eps-Greedy algorithm with sample average Q estimate updates\n",
    "    Then run the eps-Greedy algorithm with constant alpha Q estimate updates\n",
    "    To grasp which one is better, we also compute and return the actual optimal Q*'s as the random walk progresses \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    print(\"eps-Greedy with eps=0.1, 2000 Runs, 10000 Steps\")\n",
    "    #standard set up but now with a random walk- eps=0.1 case (no UCB), now with random walk\n",
    "    \n",
    "    bandit_eps = ArmedBanditTB(nRuns = steps, q_init=0, eps=0.1, rw=True)\n",
    "    \n",
    "    rewards_eps = np.zeros(shape=(steps,runs))\n",
    "    history_eps = np.zeros(shape=(steps,runs))\n",
    "    correct_ratio_eps = np.zeros(shape=(steps,runs))\n",
    "    \n",
    "    #carry out runs and store results accordingly\n",
    "    for i in range(runs):\n",
    "        if verbose!=0:\n",
    "            if i == runs-1:\n",
    "                print(\"Run %i Completed\" %runs)\n",
    "            elif i == 0:\n",
    "                print(\"Run 001 Completed\")\n",
    "            elif i%250 == 0:\n",
    "                print(\"Run %i Completed\" %i)\n",
    "            \n",
    "            bandit_eps.reset()\n",
    "            bandit_eps.runTrials()\n",
    "            rewards_eps[:,i]=np.asarray(bandit_eps.rewards)\n",
    "            correct_ratio_eps[:,i]=np.asarray(bandit_eps.count_correct)\n",
    "            \n",
    "            #take max reward at time and store it accordingly- maximum reward could've been attained as per Q*\n",
    "            history_eps[:,i]=np.max(bandit_eps.q_optimals_history,axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #now doing the same in the second case but also initialising with a constant alpha, no UCB again\n",
    "    print(\"\\neps-Greedy with eps=0.1, 2000 Runs, 10000 Steps, Constant Alpha=0.1\")\n",
    "    bandit_alpha = ArmedBanditTB(nRuns=steps, q_init=0, eps=0.1, rw= True, alpha_const=True, alpha=0.1)\n",
    "    rewards_alpha = np.zeros(shape=(steps, runs))\n",
    "    history_alpha = np.zeros(shape=(steps,runs))\n",
    "    correct_ratio_alpha = np.zeros(shape=(steps,runs))\n",
    "    \n",
    "    for i in range(runs):\n",
    "        if verbose!=0:\n",
    "            if i == runs-1:\n",
    "                print(\"Run %i Completed\" %runs)\n",
    "            elif i == 0:\n",
    "                print(\"Run 001 Completed\")\n",
    "            elif i%250 == 0:\n",
    "                print(\"Run %i Completed\" %i)\n",
    "                \n",
    "        bandit_alpha.reset()\n",
    "        bandit_alpha.runTrials()\n",
    "        rewards_alpha[:,i]=np.asarray(bandit_alpha.rewards)\n",
    "        history_alpha[:,i]=np.max(bandit_alpha.q_optimals_history,axis=0)\n",
    "        correct_ratio_alpha[:,i]=np.asarray(bandit_alpha.count_correct)\n",
    "    \n",
    "    return rewards_eps, rewards_alpha, history_eps, history_alpha, correct_ratio_eps, correct_ratio_alpha\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0802dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtaining results for both tests\n",
    "rw_rewards_eps, rw_rewards_alpha, rw_history_eps, rw_history_alpha, rw_ratio_eps, rw_ratio_alpha = playBanditNonStationary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d8612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#producing plots required for both tests\n",
    "fig2, (ax3, ax4) = plt.subplots(2,1,figsize=(10,8))\n",
    "ax3.plot(rw_rewards_eps.mean(axis=1), label=\"Sample Average Method\")\n",
    "ax3.plot(rw_rewards_alpha.mean(axis=1), label=\"Constant Alpha = 0.1\")\n",
    "ax3.plot(rw_history_eps.mean(axis=1), label=\"Maximum Possible Reward\")\n",
    "ax3.set_xlabel(\"Steps\", fontsize=12)\n",
    "ax3.set_ylabel(\"Average Reward\", fontsize=12)\n",
    "ax3.set_title(\"Sample Average Method vs Constant Alpha Method vs Max Attainable Reward\", fontsize=12)\n",
    "ax3.legend()\n",
    "\n",
    "ax4.plot(100*rw_ratio_eps.mean(axis=1), label=\"Sample Average Method\")\n",
    "ax4.plot(100*rw_ratio_alpha.mean(axis=1), label=\"Constant Alpha\")\n",
    "ax4.set_xlabel(\"Steps\", fontsize=12)\n",
    "ax4.set_ylabel(\"Optimal Action (%)\", fontsize = 12)\n",
    "ax4.set_title(\"Sample Average Method vs Constant Alpha Method\", fontsize=12)\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
